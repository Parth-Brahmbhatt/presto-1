#
# WARNING
# ^^^^^^^
# This configuration file is for development only and should NOT be used be
# used in production. For example configuration, see the Presto documentation.
#
connector.name=iceberg

#Metastore properties
hive.metastore-cache-ttl=1m
hive.metastore-refresh-interval=1m
hive.metastore.uri=thrift://localhost:9083
hive.assume-canonical-partition-keys=true
hive.metastore-timeout=20s
hive.non-managed-table-writes-enabled=true

#S3 properties
hive.s3.staging-directory=/mnt/tmp/
hive.s3.max-client-retries=25
hive.s3.max-error-retries=50
hive.s3.max-connections=500
hive.s3.connect-timeout=5s
hive.s3.socket-timeout=5s
<<<<<<< HEAD
hive.s3.max-backoff-time=10s
hive.s3.max-retry-time=1m
hive.config.resources=etc/hive-site.xml
hive.allow-drop-table=true
hive.allow-rename-table=true
hive.allow-add-column=true
hive.allow-drop-column=true
hive.allow-rename-column=true
=======
hive.s3.max-backoff-time=10m
hive.s3.max-retry-time=10m

#Vault support properties
#hive.security=netflix
hive.s3.role.mappings=vault=arn:aws:iam::219382154434:role/s3_all_with_vault,vault_events=arn:aws:iam::219382154434:role/s3_all_with_vault,vault_stg=arn:aws:iam::219382154434:role/s3_all_with_vault,restricted_use_mkt=arn:aws:iam::219382154434:role/RestrictedUseMKTRole,cldsec=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_stg=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_test=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_vault=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_vault_stg=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_stg=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole,cldsec_test=arn:aws:iam::219382154434:role/S3CldSecReadOnlyRole
hive.s3.use-instance-credentials=false
hive.config.resources=etc/core-site.xml

#Parquet properties
hive.storage-format=PARQUET
hive.parquet.use-column-names=false
hive.max-split-size=128MB
hive.parquet-optimized-reader.enabled=true
hive.parquet-predicate-pushdown.enabled=true
#TODO set hive.force-local-scheduling=true once we IcebergSplit handle the hdfs block locations
#and it can start returning the correct address for a split. This is required for HDFSCache to work.
hive.force-local-scheduling=false

#iceberg properties
iceberg.metacat-rest-endpoint=http://metacat.dynprod.netflix.net:7001
iceberg.metacat-catalog-name=prodhive
iceberg.metastore-warehouse-dir=s3n://netflix-dataoven-prod-users/hive/warehouse
>>>>>>> 4d2e776277... Setting the table location based on location service by deriving the DB and table default location and adding hive.warehouse.dir as an iceberg config so we can pass it down to iceberg libraries which relies on these configs when a default location is not returned by metastore.

